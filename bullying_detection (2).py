# -*- coding: utf-8 -*-
"""Bullying_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TYW9G0AZDXVQf3GFZPWLC2DwXNS7zrUZ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("dataset.csv")

df.shape

df.head()

df['label'].unique()

# Data Manipulaton

def perform_data_manipulation():
    df = pd.read_csv("dataset.csv")

    for index in df.index:
        if df.loc[index, "label"]==-1:
            df.loc[index, "label"] = 1
    return df

df = perform_data_manipulation()

df.head()

df['label'].unique()

def perform_data_manipulation():
    df = pd.read_csv("dataset.csv")

    for index in df.index:
        if df.loc[index, "label"]==-1:
            df.loc[index, "label"] = 1
    return df

df = perform_data_manipulation()

df.head()

df['label'].unique()

df.shape[0]

"""#Classification"""

def performdatadistribution(df):
    total = df.shape[0]
    num_non_toxic = df[df['label']==0].shape[0]
    slices = [num_non_toxic/total, (total-num_non_toxic)/total]
    labeling = ['Non-Toxic', 'Toxic']
    explode = [0.2, 0]
    plt.pie(slices, explode = explode, shadow=True, autopct="%1.1f%%", labels = labeling, wedgeprops={'edgecolor': 'black'})
    plt.title('Number of Toxic Vs Non- Toxic Test Sample')
    plt.tight_layout()

    plt.show()

performdatadistribution(df)

def remove_pattern(input_txt, pattern):
    if (type(input_txt)==str):
        r = re.findall(pattern, input_txt)
        for i in r:
            input_txt = re.sub(i, '', input_txt)
        return input_txt
    else:
        return ""

df.head(1)

import re

import nltk

def remove_pattern(input_txt, pattern):
    if isinstance(input_txt, str):
        r = re.findall(pattern, input_txt)
        for i in r:
            input_txt = re.sub(i, '', input_txt)
    return input_txt

def datasetCleaning(df):
    df['length_headline'] = df['headline'].str.len()
    combined_df = pd.concat([df, df], ignore_index=True)

    combined_df['tidy_tweet'] = np.vectorize(remove_pattern)(
        combined_df['headline'], "@[\\w]*"
    )
    combined_df['tidy_tweet'] = combined_df['tidy_tweet'].str.replace(
        "[^a-zA-Z#]", " ", regex=True
    )
    combined_df['tidy_tweet'] = combined_df['tidy_tweet'].apply(
        lambda x: ' '.join([w for w in x.split() if len(w) > 3])
    )
    combined_df['length_tidy_tweet'] = combined_df['tidy_tweet'].str.len()


    tokenized_tweet = combined_df['tidy_tweet'].apply(lambda x: x.split())

    nltk.download('wordnet', quiet=True)

    lemmatizer = nltk.stem.WordNetLemmatizer()

    tokenized_tweet = tokenized_tweet.apply(
        lambda x: [lemmatizer.lemmatize(i) for i in x]
    )

    combined_df['tidy_tweet'] = tokenized_tweet.apply(lambda x: ' '.join(x))

    return combined_df, df

combined_df, df =datasetCleaning(df)

combined_df.head()

# Dataset splitting

from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer
from nltk import sent_tokenize, word_tokenize

from sklearn.model_selection import train_test_split

def performdatasplit(x, y, combined_df, df):
    X_train, X_test, y_train, y_test = train_test_split(combined_df['tidy_tweet'], combined_df['label'], test_size = x, random_state = y)
    print(f"Number of rows in the total dataset: {combined_df.shape[0]}")
    print(f"Number of rows in the train dataset: {X_train.shape[0]}")
    print(f"Number of rows in the test dataset: {X_test.shape[0]}")

    files = open("stopwords.txt" , "r")
    content = files.read()
    content_list = content.split("\n")
    files.close()

    tfidfvector = TfidfVectorizer(stop_words=content_list, lowercase=True)

    training_data = tfidfvector.fit_transform(X_train.values.astype('U'))

    testing_data = tfidfvector.transform(X_test.values.astype('U'))

    filename = 'tfidfvectoizer.pkl'

    pickle.dump(tfidfvector.vocabulary_, open(filename, 'wb'))

    return X_train , X_test, y_train, y_test, testing_data, filename, training_data, content_list

import pickle

X_train , X_test, y_train, y_test, testing_data, filename, training_data, content_list = performdatasplit(0.2, 42, combined_df, df)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

lr = LogisticRegression(max_iter=1000)
lr.fit(training_data, y_train)

y_pred = lr.predict(testing_data)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""#LSTM model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

max_words = 5000     # maximum number of words in vocabulary
max_len = 100

lstm_model = Sequential()
lstm_model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))
lstm_model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
lstm_model.add(Dense(1, activation='sigmoid'))

lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

lstm_model.summary()

tokenizer = Tokenizer(num_words=max_words, lower=True)
tokenizer.fit_on_texts(combined_df['tidy_tweet'])

X = tokenizer.texts_to_sequences(combined_df['tidy_tweet'])
X = pad_sequences(X, maxlen=max_len)

y = np.array(combined_df['label'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

history_lstm = lstm_model.fit(
    X_train, y_train,
    epochs=5,
    batch_size=64,
    validation_data=(X_test, y_test),
    verbose=1
)

y_pred_lstm = (lstm_model.predict(X_test) > 0.5).astype("int32")

print("LSTM Model Accuracy:", accuracy_score(y_test, y_pred_lstm))
print(classification_report(y_test, y_pred_lstm))

plt.plot(history_lstm.history['accuracy'], label='train acc')
plt.plot(history_lstm.history['val_accuracy'], label='val acc')
plt.legend()
plt.show()

"""# CNN MODEL"""

cnn_model = Sequential()
cnn_model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))
cnn_model.add(Conv1D(128, 5, activation='relu'))
cnn_model.add(MaxPooling1D(pool_size=2))
cnn_model.add(Flatten())
cnn_model.add(Dense(64, activation='relu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(1, activation='sigmoid'))

cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

cnn_model.summary()

history_cnn = cnn_model.fit(
    X_train, y_train,
    epochs=5,
    batch_size=64,
    validation_data=(X_test, y_test),
    verbose=1
)

y_pred_cnn = (cnn_model.predict(X_test) > 0.5).astype("int32")

print("CNN Model Accuracy:", accuracy_score(y_test, y_pred_cnn))
print(classification_report(y_test, y_pred_cnn))

plt.plot(history_cnn.history['accuracy'], label='train acc')
plt.plot(history_cnn.history['val_accuracy'], label='val acc')
plt.legend()
plt.show()

"""#CNN + LSTM Hybrid Architecture

"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout

# CNN + LSTM Hybrid Model
hybrid_model = Sequential()

# Embedding Layer
hybrid_model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))

# CNN layers
hybrid_model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
hybrid_model.add(MaxPooling1D(pool_size=2))

# LSTM layer
hybrid_model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))

# Dense layers
hybrid_model.add(Dense(64, activation='relu'))
hybrid_model.add(Dropout(0.5))
hybrid_model.add(Dense(1, activation='sigmoid'))

# Compile
hybrid_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

hybrid_model.summary()

history_hybrid = hybrid_model.fit(
    X_train, y_train,
    epochs=6,
    batch_size=64,
    validation_data=(X_test, y_test),
    verbose=1
)

from sklearn.metrics import accuracy_score, classification_report

y_pred_hybrid = (hybrid_model.predict(X_test) > 0.5).astype("int32")

print("CNN + LSTM Model Accuracy:", accuracy_score(y_test, y_pred_hybrid))
print(classification_report(y_test, y_pred_hybrid))

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 4))
plt.plot(history_hybrid.history['accuracy'], label='Train Accuracy')
plt.plot(history_hybrid.history['val_accuracy'], label='Validation Accuracy')
plt.title('CNN + LSTM Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.figure(figsize=(8, 4))
plt.plot(history_hybrid.history['loss'], label='Train Loss')
plt.plot(history_hybrid.history['val_loss'], label='Validation Loss')
plt.title('CNN + LSTM Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

hybrid_model.save("hybrid_cnn_lstm_bullying_model.h5")
pickle.dump(tokenizer, open("tokenizer.pkl", "wb"))

def predict_text_hybrid(text):
    seq = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=max_len)
    pred = (hybrid_model.predict(padded) > 0.5).astype("int32")
    return "Toxic" if pred == 1 else "Non-Toxic"

print(predict_text_hybrid("I hate you, you are the worst!"))
print(predict_text_hybrid("Have a great day my friend!"))